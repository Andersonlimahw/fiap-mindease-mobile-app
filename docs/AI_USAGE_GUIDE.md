# Guia de Uso: Sistema H√≠brido de IA

## Para Desenvolvedores

### Testar localmente com Ollama

```bash
# 1. Instalar Ollama (macOS/Linux/Windows)
# https://ollama.ai

# 2. Rodar Ollama em background
ollama serve

# 3. Pull do modelo (primeira vez)
ollama pull llama3

# 4. Rodar app apontando para Ollama local
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=ollama \
EXPO_PUBLIC_AI_OLLAMA_URL=http://localhost:11434 \
npm run ios
```

### Testar com Firebase Cloud Function

```bash
# 1. Setup Firebase (j√° deve estar pronto)
# 2. Criar Cloud Function que chama Ollama/OpenAI
# (Documenta√ß√£o em FIREBASE_INTEGRATION.md)

# 3. Rodar app em modo cloud
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=firebase \
npm run ios
```

### Testar com Torch (quando dispon√≠vel)

```bash
# 1. Certifique-se que expo-torch est√° instalado
npm list expo-torch

# 2. Copiar modelo para projeto
# cp modelo.pt src/data/torch/models/

# 3. Habilitar torch
EXPO_PUBLIC_AI_TORCH_ENABLED=true \
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=torch \
npm run ios
```

### Debug: Ver qual reposit√≥rio est√° sendo usado

```typescript
// Em qualquer componente ou hook
import { getRepositorySelector } from '@app/store/diStore';

const selector = getRepositorySelector();

// Ver stats
console.table(selector.getStats());

// Ver √∫ltimo sucesso
const attempts = selector.getAttempts();
const last = attempts[attempts.length - 1];
console.log(`√öltimo: ${last.name} - ${last.success ? '‚úÖ' : '‚ùå'} em ${last.latencyMs}ms`);

// Limpar cache
selector.clearCache();

// Reseta stats
selector.resetStats();
```

## Para Designers/UX

### Indicadores Visuais de Resposta

A origem da resposta √© mostrada automaticamente:

```tsx
// Componente j√° integrado no ChatScreen
<AIStatusIndicator metadata={responseMetadata} />
```

**Cores padr√£o:**
- üü¢ Verde (#00DD88): Local (Torch) - mais r√°pido, <500ms
- üü¶ Teal (#00AA88): Cache - instant√¢neo, 0ms
- üîµ Azul (#0088FF): Local Dev (Ollama) - r√°pido, <2s
- üü† Laranja (#FF8800): Cloud (Firebase) - normal, <3s
- ‚ö´ Cinza (#888888): Demo/Fallback - sempre funciona

### Typing Indicator

Mostra que a IA est√° processando:

```tsx
<AITypingIndicator visible={isLoading} source="Cloud" />
```

Vem com anima√ß√£o de pontos bouncantes + texto din√¢mico.

### Estados Poss√≠veis

1. **Waiting**: Input vazio
   - Nada vis√≠vel

2. **Typing**: User digitando
   - Bot√£o send destacado

3. **Loading**: App processando
   - `<AITypingIndicator visible={true} source="..." />`
   - Spinner/skeleton opcional

4. **Response Received**: Resposta do IA
   - Mensagem renderizada
   - `<AIStatusIndicator metadata={...} />`
   - Indicador de cache se reutilizada

## Para Product

### M√©tricas Implementadas

O sistema rastreia automaticamente:

```typescript
interface RepositoryStats {
  name: string;                  // "torch" | "ollama" | "firebase" | "demo"
  totalAttempts: number;         // Quantas vezes foi tentado
  successCount: number;          // Quantas tiveram sucesso
  averageLatencyMs: number;      // Lat√™ncia m√©dia
  successRate: number;           // 0.0 a 1.0 (%)
  lastUsedAt: number;            // Timestamp
}
```

### Analytics Dashboard (Future)

Podemos expor esses dados para:
- **Performance Monitoring**: Qual reposit√≥rio √© mais r√°pido?
- **Reliability**: Qual tem maior taxa de sucesso?
- **Cost Analysis**: Quantas vezes usou Cloud vs Local?
- **User Experience**: Cache hit rate?

### SLAs para Resposta

Alvo de performance por ambiente:

| Reposit√≥rio | Target | Atual | Status |
|---|---|---|---|
| Torch | <500ms | ~300ms | ‚úÖ On track |
| Ollama | <2s | ~1.2s | ‚úÖ Good |
| Firebase | <3s | ~1.8s* | ‚úÖ Good |
| Demo | <1s | ~800ms | ‚úÖ Fallback |

*Depende da Cloud Function estar otimizada

## Para QA / Testing

### Casos de Teste Implementados

1. **Happy Path**: User ‚Üí Torch ‚Üí Response ‚úÖ
2. **Fallback Chain**: Torch fails ‚Üí Ollama ‚Üí Response ‚úÖ
3. **Cache Hit**: Mesma pergunta ‚Üí Instant response ‚úÖ
4. **Offline**: App funciona com respostas demo ‚úÖ
5. **Timeout**: Reposit√≥rio demora ‚Üí fallback autom√°tico ‚úÖ

### Testar Cen√°rios

```typescript
// For√ßar timeout
EXPO_PUBLIC_AI_TORCH_TIMEOUT=100 npm run ios  // Vai cair para ollama

// Desabilitar torch
EXPO_PUBLIC_AI_TORCH_ENABLED=false npm run ios

// Modo mock s√≥ (demo responses)
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=mock npm run ios

// Ver logs detalhados
__DEV__ = true npm run ios  // Console.log habilitado
```

### Endpoints de API (Firebase)

Se implementando Cloud Function, testar:

```bash
# Test request
curl -X POST https://region-project.cloudfunctions.net/chatHandler \
  -H "Content-Type: application/json" \
  -d '{
    "userId": "user123",
    "messages": [{"role": "user", "content": "Como usar Pomodoro?"}],
    "systemPrompt": "Voc√™ √© assistente de produtividade"
  }'

# Expected response
{
  "success": true,
  "content": "A T√©cnica Pomodoro...",
  "source": "ollama",
  "latencyMs": 1200,
  "model": "llama3"
}
```

## Troubleshooting

### Chat n√£o responde
1. Verificar qual reposit√≥rio est√° ativo: `selector.getStats()`
2. Se Torch: verificar se modelo foi carregado
3. Se Ollama: verificar se servidor est√° rodando (`curl http://localhost:11434/api/tags`)
4. Se Firebase: verificar Cloud Function logs
5. Fallback deve sempre retornar demo response

### Respostas lentas
1. Verificar lat√™ncia por reposit√≥rio: `selector.getStats()`
2. Se >3s: considerar otimizar modelo ou trocar para Torch
3. Se Ollama: aumentar RAM/GPU
4. Se Firebase: otimizar Cloud Function

### Cache n√£o funciona
1. Limpar cache: `selector.clearCache()`
2. Verificar TTL: 3600000ms (1 hora)
3. Mesma pergunta deve usar cache

### Reposit√≥rio n√£o carrega
1. Ver logs: `__DEV__` console
2. Verificar AppConfig: `AppConfig.ai`
3. Verificar env vars: `EXPO_PUBLIC_AI_*`
4. Fallback para demo deve sempre funcionar

## Links √öteis

- [AI Architecture Documentation](./AI_ARCHITECTURE.md)
- [Firebase Integration](./FIREBASE_INTEGRATION.md)
- [AppConfig](../src/config/appConfig.ts)
- [RepositorySelector](../src/core/ai/RepositorySelector.ts)
- [UIComponents](../src/presentation/components/)
