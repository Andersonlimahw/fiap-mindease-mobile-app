# Arquitetura HÃ­brida de IA - MindEase Mobile

## ğŸ“Š VisÃ£o Geral

O MindEase Mobile implementa um sistema inteligente de IA com mÃºltiplas estratÃ©gias de fallback, garantindo melhor performance e experiÃªncia do usuÃ¡rio:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           User sends message (ChatScreen)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  RepositorySelector     â”‚
                    â”‚  (Strategy + Cache)     â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Try repositories in order:                 â”‚
        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚ 1ï¸âƒ£ TorchChatRepository (on-device)         â”‚
        â”‚    â”œâ”€ <500ms (modelo leve)                  â”‚
        â”‚    â””â”€ Offline funcionando                   â”‚
        â”‚                                             â”‚
        â”‚ 2ï¸âƒ£ OllamaChatRepository (dev server)       â”‚
        â”‚    â”œâ”€ <2s (llama3 local)                    â”‚
        â”‚    â””â”€ Qualidade high                        â”‚
        â”‚                                             â”‚
        â”‚ 3ï¸âƒ£ FirebaseChatRepository + Cloud Function â”‚
        â”‚    â”œâ”€ <3s (nuvem)                          â”‚
        â”‚    â””â”€ Melhor qualidade                      â”‚
        â”‚                                             â”‚
        â”‚ 4ï¸âƒ£ MockChatRepository (demo)               â”‚
        â”‚    â”œâ”€ <1s (hardcoded)                       â”‚
        â”‚    â””â”€ Sempre funciona                       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ Return with metadata:       â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ â€¢ source (torch/cloud/demo) â”‚
                    â”‚ â€¢ latencyMs                 â”‚
                    â”‚ â€¢ cached (sim/nÃ£o)          â”‚
                    â”‚ â€¢ model name                â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  Display with indicator:    â”‚
                    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                    â”‚ ğŸŸ¢ Local (torch)           â”‚
                    â”‚ ğŸ”µ Local Dev (ollama)      â”‚
                    â”‚ ğŸŸ  Cloud (firebase)        â”‚
                    â”‚ âšª Demo (fallback)         â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Componentes Principais

### 1. **RepositorySelector** (`src/core/ai/RepositorySelector.ts`)
- Orquestra fallback chain
- Implementa retry com timeout
- Cache de respostas com TTL
- Rastreamento de stats por repositÃ³rio

**Responsabilidades:**
- Tentar repositÃ³rio principal primeiro
- Fallback automÃ¡tico em timeout/erro
- Cachear respostas bem-sucedidas (1 hora TTL)
- Registrar mÃ©tricas (latÃªncia, taxa de sucesso)

### 2. **TorchChatRepository** (`src/data/torch/TorchChatRepository.ts`)
- Executa modelos PyTorch localmente (when ready)
- On-device inference, sem conectividade
- Fallback para respostas demo se modelo nÃ£o carregar

**Status Atual:**
- âœ… Estrutura criada
- â³ Aguardando disponibilidade de expo-torch estÃ¡vel
- ğŸ“‹ TODOs para integraÃ§Ã£o de modelo real

### 3. **AppConfig Expandido** (`src/config/appConfig.ts`)
- ConfiguraÃ§Ã£o centralizada de estratÃ©gia de IA
- Suporte a mÃºltiplos repositÃ³rios
- Timeouts por repositÃ³rio
- Feature flags

**VariÃ¡veis de ambiente:**
```bash
# EstratÃ©gia primÃ¡ria
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=firebase  # torch|ollama|firebase|mock

# Torch config
EXPO_PUBLIC_AI_TORCH_ENABLED=true
EXPO_PUBLIC_AI_TORCH_MODEL=distilbert-base-multilingual-cased
EXPO_PUBLIC_AI_TORCH_MODEL_URL=https://...

# Ollama config (dev)
EXPO_PUBLIC_AI_OLLAMA_URL=http://localhost:11434
EXPO_PUBLIC_AI_OLLAMA_MODEL=llama3
```

### 4. **UI Components** (Phase 4)

#### AIStatusIndicator
```tsx
<AIStatusIndicator metadata={responseMetadata} />
```
- Mostra origem (Local/Cloud/Demo)
- Exibe latÃªncia
- Indica se foi cacheado
- Cores visuais por fonte

#### AITypingIndicator
```tsx
<AITypingIndicator visible={isLoading} source="Cloud" />
```
- AnimaÃ§Ã£o de digitaÃ§Ã£o
- Mostra qual repositÃ³rio estÃ¡ processando
- Feedback visual para o usuÃ¡rio

### 5. **DI Container Integrado** (`src/store/diStore.ts`)
```typescript
// Automaticamente cria:
// - TorchChatRepository
// - OllamaChatRepository
// - FirebaseChatRepository  
// - MockChatRepository
// E envolve com RepositorySelector

// Wrapper invisÃ­vel ao usuÃ¡rio - mesmo contrato de ChatRepository
container.set(TOKENS.ChatRepository, wrappedRepo);
```

## ğŸ“ Tipos e Interfaces

```typescript
enum AIResponseSource {
  LOCAL = 'local',              // TorchChatRepository
  LOCAL_CACHED = 'local_cached', // Cache
  OLLAMA = 'ollama',            // Dev server
  CLOUD = 'cloud',              // Firebase
  DEMO = 'demo',                // Fallback
}

interface AIResponseMetadata {
  source: AIResponseSource;
  latencyMs: number;
  cached: boolean;
  confidence?: number;
  model?: string;
  timestamp: number;
}

interface RepositoryStats {
  name: string;
  totalAttempts: number;
  successCount: number;
  averageLatencyMs: number;
  successRate: number; // 0-1
}
```

## ğŸ”„ Fluxo de ExecuÃ§Ã£o

### 1. User sends message
```typescript
const { sendMessage } = useChatActions();
await sendMessage("Como usar Pomodoro?");
```

### 2. Store resolves repository
```typescript
const repo = useDIStore.getState().di.resolve(TOKENS.ChatRepository);
// Returns wrapped repo with RepositorySelector
```

### 3. Selector tries in order
```typescript
try {
  // 1. Try Torch (timeout 3s)
  response = await torchRepo.sendMessage(userId, messages, systemPrompt);
} catch {
  try {
    // 2. Try Ollama (timeout 30s)
    response = await ollamaRepo.sendMessage(userId, messages, systemPrompt);
  } catch {
    // 3. Try Firebase Cloud Function (timeout 10s)
    response = await firebaseRepo.sendMessage(userId, messages, systemPrompt);
  }
}
// 4. If all fail, fallback to demo responses
```

### 4. Return with metadata
```typescript
return {
  response: { content: "..." },
  metadata: {
    source: 'torch',
    latencyMs: 350,
    cached: false,
    timestamp: Date.now(),
  }
};
```

### 5. UI displays with indicator
```tsx
<ChatBubble message={message} />
<AIStatusIndicator metadata={metadata} /> {/* ğŸŸ¢ Local â€¢ 350ms */}
```

## ğŸ¯ ConfiguraÃ§Ã£o por Ambiente

### Development
```env
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=ollama
EXPO_PUBLIC_AI_TORCH_ENABLED=true
EXPO_PUBLIC_AI_OLLAMA_URL=http://localhost:11434
```
Cadeia: Torch â†’ Ollama â†’ Firebase â†’ Demo

### Production  
```env
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=firebase
EXPO_PUBLIC_AI_TORCH_ENABLED=true
```
Cadeia: Torch â†’ Firebase â†’ Demo

### Testing/Demo
```env
EXPO_PUBLIC_AI_PRIMARY_REPOSITORY=mock
EXPO_PUBLIC_AI_TORCH_ENABLED=false
```
Cadeia: Demo (instantÃ¢neo)

## ğŸ“Š Monitoring e Debugging

### Acessar stats do repositÃ³rio
```typescript
import { getRepositorySelector } from '@app/store/diStore';

const selector = getRepositorySelector();
const stats = selector.getStats();
console.table(stats);

// Output:
// â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
// â”‚ name    â”‚ source   â”‚ total  â”‚ success  â”‚ avgLatency â”‚
// â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
// â”‚ torch   â”‚ local    â”‚ 15     â”‚ 12       â”‚ 350ms      â”‚
// â”‚ ollama  â”‚ ollama   â”‚ 3      â”‚ 3        â”‚ 1200ms     â”‚
// â”‚ firebaseâ”‚ cloud    â”‚ 0      â”‚ 0        â”‚ 0ms        â”‚
// â”‚ demo    â”‚ demo     â”‚ 0      â”‚ 0        â”‚ 0ms        â”‚
// â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Log de tentativas
```typescript
const attempts = selector.getAttempts();
attempts.forEach(att => {
  console.log(`${att.name}: ${att.success ? 'âœ…' : 'âŒ'} (${att.latencyMs}ms)`);
});
```

## ğŸš€ PrÃ³ximos Passos

### Phase 2.0 (3-6 meses)
- âœ… Implementar modelo real no TorchChatRepository
- âœ… Fine-tuning com dataset portuguÃªs
- âœ… OtimizaÃ§Ã£o de tamanho (quantization)
- âœ… A/B testing: Torch vs Cloud

### Phase 3.0 (6-12 meses)
- Modelo especializado para produtividade
- Multi-turn conversation com contexto
- Persistent context em Firebase
- Fine-tuning com dados de usuÃ¡rios

### Phase 4.0+ (Futuro)
- Streaming responses para modelos maiores
- Voice input/output (TTS)
- RAG com documentos de ajuda
- RecomendaÃ§Ãµes personalizadas

## âš ï¸ ConsideraÃ§Ãµes Importantes

### Quando Torch serÃ¡ pronto
expo-torch estÃ¡ em beta. Status:
- iOS: suporte bÃ¡sico, libtorch disponÃ­vel
- Android: PyTorch Mobile funcional
- Performance: <500ms em modelos leves confirmado
- DocumentaÃ§Ã£o: ainda em desenvolvimento

### Plano B se Torch nÃ£o funcionar
Se expo-torch nÃ£o chegar a production-ready:
1. Usar `react-native-pytorch` como alternativa
2. Manter Ollama como primary (mesmo custo infra)
3. Continuar com Cloud Function fallback

### SeguranÃ§a
- Dados do usuÃ¡rio nunca deixam dispositivo no Torch
- Mensagens cacheadas apenas localmente
- Firebase usa security rules por userId
- Sem anÃ¡lise de dados de conversa para publicidade

## ğŸ“š ReferÃªncias

- AppConfig: `src/config/appConfig.ts`
- RepositorySelector: `src/core/ai/RepositorySelector.ts`
- TorchChatRepository: `src/data/torch/TorchChatRepository.ts`
- Types: `src/types/ai.ts`
- Components: `src/presentation/components/AIStatusIndicator.tsx`
- Store: `src/store/diStore.ts`

## ğŸ”— Arquitetura Relacionada

- [Firebase Integration Guide](./FIREBASE_INTEGRATION.md)
- [Clean Architecture](../CLAUDE.md#architecture)
- [DI Container](../src/core/di/container.tsx)
- [Chat Store](../src/store/chatStore.ts)
